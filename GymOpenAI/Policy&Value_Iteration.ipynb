{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy&Value_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uYYAV3B65zo"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg1wlNLe0PJf"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iwfwLQbVfv2"
      },
      "source": [
        "# Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3CKewoy1Uu4"
      },
      "source": [
        "def value_iteration(env, max_iters=1000, gamma=0.9):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Compute the value for state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            # Compute the q-value for each action\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Select the best action\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            # Converged iteration at time complexity O(|S|) \n",
        "            print(f'Converged at {(i+2)*env.action_space.n}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmQUE53V1btH"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    # Compute the best action for each state in the game\n",
        "    # Compute q-value for each (state-action) pair in the game\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # Compute q_value for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # Select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPyDzEVgVXH3"
      },
      "source": [
        "# Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1X4Evfjc7U4"
      },
      "source": [
        "def policy_evaluation(env, v_values, policy, max_iters=1000, gamma=0.9):\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "        for state in range(env.observation_space.n):\n",
        "            v_values[state] = 0\n",
        "            for prob, next_state, reward, done in env.P[state][policy[state]]:\n",
        "                v_values[state] += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            break\n",
        "\n",
        "    return i+1, v_values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBpaFPD-Inr0"
      },
      "source": [
        "def policy_improvement(env, v_values, policy, gamma=0.9):\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # Compute Q_value for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # Selecting action haved best Q_values\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vZ-sBMYcHxU"
      },
      "source": [
        "def policy_iteration(env, max_loops=1000, max_eva_iters=1000, gamma=0.9):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "    iterations = 0\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        iters, v_values = policy_evaluation(env, v_values, policy, max_eva_iters, gamma)\n",
        "        prev_policy, policy = np.copy(policy), policy_improvement(env, v_values, policy, gamma)\n",
        "\n",
        "        iterations += iters + env.action_space.n\n",
        "\n",
        "        if np.all(np.isclose(policy, prev_policy)):\n",
        "            # Converged iteration at time complexity O(|S|)\n",
        "            print(f'Converged at {iterations}-th iterations.')\n",
        "            break\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc5-l1QrU5Wp"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFJgjMAe1k-B"
      },
      "source": [
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    #time.sleep(1)\n",
        "    #display.clear_output(wait=True)\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        #print(f'Step {steps}')\n",
        "        #env.render()\n",
        "        #time.sleep(0.2)\n",
        "        #if not done:\n",
        "        #    display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TbG_9bZ1r2W"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes=1000):\n",
        "    reward = []\n",
        "    success = 0\n",
        "\n",
        "    for i in range(max_episodes):\n",
        "        reward.append(play(env, policy))\n",
        "\n",
        "        if reward[i] > 0:\n",
        "            success += 1\n",
        "    \n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Reward-average: {np.average(reward)}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cr4Y2yfvyvY"
      },
      "source": [
        "## FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRF8PEEB08Wr"
      },
      "source": [
        "env1 = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI-XpBg-1VsE",
        "outputId": "4c9cc036-6784-4291-938a-82ed6ed3b634"
      },
      "source": [
        "''' Value Iteration '''\n",
        "start = time.time()\n",
        "v_values_00 = value_iteration(env1, max_iters=1000, gamma=0.9)\n",
        "policy_00 = policy_extraction(env1, v_values_00, gamma=0.9)\n",
        "print(f'Runtime: {time.time() - start}')\n",
        "play_multiple_times(env1, policy_00, 1000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 324-th iteration.\n",
            "Runtime: 0.06965208053588867\n",
            "Number of successes: 725/1000\n",
            "Reward-average: 0.725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPl515btkmmj",
        "outputId": "1faaad12-7f96-472c-d7cd-e40f74e43fc5"
      },
      "source": [
        "''' Policy Iteration '''\n",
        "start = time.time()\n",
        "policy_01 = policy_iteration(env1)\n",
        "print(f'Runtime: {time.time() - start}')\n",
        "play_multiple_times(env1, policy_01, 1000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 256-th iterations.\n",
            "Runtime: 0.0457158088684082\n",
            "Number of successes: 732/1000\n",
            "Reward-average: 0.732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyecAbkN2Uy_",
        "outputId": "31e3d854-2da6-4c58-e72c-d90307283a3d"
      },
      "source": [
        "''' Policy comparation '''\n",
        "#print(policy_00)\n",
        "#print(policy_01)\n",
        "print(np.where(policy_00 != policy_01))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([], dtype=int64),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaEI_iKWwHVN"
      },
      "source": [
        "## FrozenLake8x8-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cVfz_LozASH"
      },
      "source": [
        "env2 = gym.make('FrozenLake8x8-v0')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GolsbHT03Es",
        "outputId": "38f59adb-c92b-4f4e-adea-73058e129735"
      },
      "source": [
        "''' Value Iteration '''\n",
        "start = time.time()\n",
        "v_values_10 = value_iteration(env2, max_iters=1000, gamma=0.9)\n",
        "policy_10 = policy_extraction(env2, v_values_10, gamma=0.9)\n",
        "print(f'Runtime: {time.time() - start}')\n",
        "play_multiple_times(env2, policy_10, 1000)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 476-th iteration.\n",
            "Runtime: 0.16405510902404785\n",
            "Number of successes: 738/1000\n",
            "Reward-average: 0.738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4LDYDXf0-1h",
        "outputId": "5f681a8e-366f-49d8-fddb-d9be1c00d8f8"
      },
      "source": [
        "''' Policy Iteration '''\n",
        "start = time.time()\n",
        "policy_11 = policy_iteration(env2)\n",
        "print(f'Runtime: {time.time() - start}')\n",
        "play_multiple_times(env2, policy_11, 1000)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 544-th iterations.\n",
            "Runtime: 0.19114303588867188\n",
            "Number of successes: 730/1000\n",
            "Reward-average: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPJ931gp4T3c",
        "outputId": "03ef58be-a2c1-44eb-9c22-dba7bdb2e919"
      },
      "source": [
        "''' Policy comparation '''\n",
        "#print(policy_10)\n",
        "#print(policy_11)\n",
        "print(np.where(policy_10 != policy_11))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([], dtype=int64),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9l1QqGwKTN"
      },
      "source": [
        "## Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwTmljFH1J0R"
      },
      "source": [
        "env3 = gym.make('Taxi-v3')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FStHd3BC1Qf4",
        "outputId": "5176f77d-d8df-4c03-e9c3-d157de9ed358"
      },
      "source": [
        "''' Value Iteration '''\n",
        "start = time.time()\n",
        "v_values_20 = value_iteration(env3, max_iters=1000, gamma=0.9)\n",
        "policy_20 = policy_extraction(env3, v_values_20, gamma=0.9)\n",
        "print(f'Runtime: {time.time() - start}')\n",
        "play_multiple_times(env3, policy_20, 1000)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 708-th iteration.\n",
            "Runtime: 1.393510341644287\n",
            "Number of successes: 1000/1000\n",
            "Reward-average: 8.016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyYRqd5r1TCZ",
        "outputId": "9b5f8c02-0d56-4999-8598-bde95429dcef"
      },
      "source": [
        "''' Policy Iteration '''\n",
        "start = time.time()\n",
        "policy_21 = policy_iteration(env3)\n",
        "print(f'Runtime: {time.time() - start}')\n",
        "play_multiple_times(env3, policy_21, 1000)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converged at 358-th iterations.\n",
            "Runtime: 0.7198879718780518\n",
            "Number of successes: 1000/1000\n",
            "Reward-average: 7.906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQl6kT8b4bga",
        "outputId": "2c0c29aa-14d5-4fe3-d05c-eefa32d1cad4"
      },
      "source": [
        "''' Policy comparation '''\n",
        "#print(policy_20)\n",
        "#print(policy_21)\n",
        "print(np.where(policy_20 != policy_21))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([], dtype=int64),)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}